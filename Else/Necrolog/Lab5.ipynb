{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "nlp = spacy.load('ru_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe8494977238f15",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crawl_next_page(url, next_button_xpath):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    next_button = soup.select_one(next_button_xpath)\n",
    "\n",
    "    # Check if there is a next page\n",
    "    if next_button:\n",
    "        next_url = next_button.get('href')\n",
    "        return next_url\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_children(token):\n",
    "    children = []\n",
    "    for child in token.children:\n",
    "        if child.pos_ != \"VERB\":\n",
    "            children.append(child)\n",
    "            children.extend(recursive_children(child))\n",
    "    return children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv(file, data):\n",
    "    with open(file, 'w', encoding='utf-8') as f:\n",
    "        for row in data:\n",
    "            f.write(row + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for extracting the necrolog from russian web site.**\n",
    "\n",
    "Code loops through the text of headlines, looking for a words associated with death. If the word is found, all the children of this word are stored(recursively). Then we look throught those children. If the child is a PER(person), we store it in a list. Then we unify those children PER, which belong to one entity in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25a304a207e47347",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Павел Карев', 'Артем Милованов', 'Алексей Жирков', 'Александр Горнов', 'Эльдар Дубровин', 'Сергей Зайцев', 'Кирилл Исанбаев', 'Денис Иванов', 'Иван', 'Максим Блохин', 'Иванов', 'Андреем Кондрашкиным']\n"
     ]
    }
   ],
   "source": [
    "url = \"https://tvtambov.ru/category/news/svo/page/\"\n",
    "heading_xpath = '//*[@id=\"posts-container\"]/li/div/h2'\n",
    "sub_heading_xpath = '//*[@id=\"posts-container\"]/li/div/p'\n",
    "death_words = [\"проститься\", \"гибель\", \"погибнуть\", \"оборваться\",\n",
    "               \"посмертно\", \"умереть\", \"скончаться\", \"покойный\", \"мертвый\", \"похоронили\"]\n",
    "page_count = 33\n",
    "necrolog = set()\n",
    "\n",
    "# Crawling through pages and scraping more headlines\n",
    "i = 1\n",
    "while url and i < page_count:\n",
    "    response = requests.get(url+str(i))\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    headlines =soup.find_all(class_='post-title')\n",
    "    sub_headlines = soup.find_all(class_='post-excerpt')\n",
    "    items = [headline.text.strip() + '. ' + sub_headline.text.strip()\n",
    "             for headline, sub_headline in zip(headlines, sub_headlines)]\n",
    "\n",
    "    for item in items:\n",
    "        doc = nlp(item)\n",
    "        necrolog_separated = list()\n",
    "        for token in doc:\n",
    "            if token.lemma_ in death_words:\n",
    "                children = recursive_children(token)\n",
    "                for child in children:\n",
    "                    if child.ent_type_ == \"PER\":\n",
    "                        necrolog_separated.append(child)\n",
    "        for ent in doc.ents:\n",
    "            if all(token in necrolog_separated for token in ent):\n",
    "                necrolog.add(ent)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "text = [ent.text for ent in necrolog]\n",
    "print(text)\n",
    "write_csv('necrolog.csv', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a sketch of the code that I used to extract the necrologies from the news website. I tested different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "097c5c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://pestrecy-rt.ru/news/tag/list/specoperaciia/\"\n",
    "# heading_xpath = 'body > main > ul > li > a > div.all-news__list_text-container > h2'\n",
    "# sub_heading_xpath = 'body > main > ul > li > a > div.all-news__list_text-container > p'\n",
    "# next_button_xpath = 'body > main > div.all-news__buttons-container > div > a'\n",
    "# text = \"Мертвый сын Валерий Иваненко с честью погиб в зоне СВО, а младший, Алексей, служит сегодня там. Он до сих пор защищает свою родину.\"\n",
    "\n",
    "# doc = nlp(text)\n",
    "# death_words = [\"проститься\", \"гибель\", \"погибнуть\", \"оборваться\",\n",
    "#                \"посмертно\", \"умереть\", \"скончаться\", \"покойный\", \"мертвый\"]\n",
    "# necrolog_separated = list()\n",
    "# necrolog = set()\n",
    "# for token in doc:\n",
    "#     if token.lemma_ in death_words:    \n",
    "#         children = recursive_children(token)\n",
    "#         # print(children)\n",
    "#         for child in children:\n",
    "#             if child.ent_type_ == \"PER\":\n",
    "#                 # print(child.text, child.ent_type_)\n",
    "#                 necrolog_separated.append(child)\n",
    "# for ent in doc.ents:\n",
    "#     if all(token in necrolog_separated for token in ent):\n",
    "#         necrolog.add(ent)\n",
    "# print([dead.text for dead in necrolog])     \n",
    "\n",
    "\n",
    "# # Iterate over the entities in the document\n",
    "# for ent in doc.ents:\n",
    "#     if ent.label_ == \"PER\":\n",
    "#         entity_tokens = [token for token in ent]\n",
    "#         entity_text = \" \".join([token.text for token in ent])\n",
    "#         # same_head = all(token.head == entity_tokens[0].head for token in entity_tokens)\n",
    "#         print(entity_text, entity_tokens[0].head.text)\n",
    "#         print([ancestor for ancestor in entity_tokens[0].ancestors])\n",
    "#         # print([child for child in entity_tokens[0].children] )\n",
    "\n",
    "# for token in doc:\n",
    "#     print(token.text, token.pos_, token.dep_)\n",
    "#     print([ancestor for ancestor in token.ancestors])\n",
    "#     print([child for child in token.children])\n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
