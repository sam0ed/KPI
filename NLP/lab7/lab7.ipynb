{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лабораторна робота №7, Обробка та аналіз текстових даних на Python, Варіант 14\n",
    "**Виконав**: студент групи ІП-11, Лошак Віктор Іванович<br>\n",
    "**Перевірив**: Юлія Тимофєєва Сергіївна<br>\n",
    "\n",
    "**Тема роботи**: Навчання моделей spaCy<br>\n",
    "**Мета роботи**: Ознайомитись з додаванням власних прикладів до моделей spaCy та компонентом для класифікації текстів.\n",
    "\n",
    "24.05.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Завдання**:<br>\n",
    "1.  Створити кілька своїх прикладів у форматі json  за тематикою: aвіарейси(англійською або українською мовою) для розпізнавання нового типу сутностей  (обрати  самостійно).  створити  програму,  що  додає  ці приклади до існуючої моделі spaCy, та навчає модель. Продемонструвати роботу. Для цього завдання використовуватимемо англійську мову. \n",
    "2.  Застосувати компонент TextCategorizer для визначення намірів. Дані для навчання  за  тематикою  варіанту обрати самостійно  або  скористатись вказаним файлом (utterance містить висловлювання, intent - намір). Дані файли містять приклади діалогів користувачів з системою-помічником за певною  тематикою,  наприклад,  замовлення  квитків  і  т.д.  Навчити компонент та продемонструвати роботу.\n",
    "\n",
    "**Task**:<br>\n",
    "1. Create several of your own examples in json format on the topic: flights\n",
    "(in English or Ukrainian) to recognize the new type entities (choose independently). Create a program that adds these examples to the existing spaCy model, and trains the model. Demonstrate execution of the program.\n",
    "We will use english for this task.\n",
    "2. Apply the TextCategorizer component to determine intent. Provide your own training data on the given topic(flights) or use flights.json(utterance contains statements, intent - intention). Given files contain examples of user dialogs with the helper system on a certain topic, for example, ticket purchase, etc.   Train the component and demonstrate execution of the program.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Example\n",
    "import random\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nlp = spacy.load(\"en_core_web_sm\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I need to check-in for flight UA345 leaving tomorrow.', {'entities': [(31, 36, 'FLIGHT_NUMBER')]})\n",
      "('Is flight DL456 delayed?', {'entities': [(9, 14, 'FLIGHT_NUMBER')]})\n",
      "('Can I get an update on AA123?', {'entities': [(22, 27, 'FLIGHT_NUMBER')]})\n",
      "('What gate is BA432 departing from?', {'entities': [(13, 18, 'FLIGHT_NUMBER')]})\n",
      "('Has flight LH678 landed yet?', {'entities': [(9, 14, 'FLIGHT_NUMBER')]})\n",
      "('Please confirm my seat on VX321.', {'entities': [(26, 31, 'FLIGHT_NUMBER')]})\n",
      "('Reservation details for flight AC980 are needed.', {'entities': [(28, 33, 'FLIGHT_NUMBER')]})\n",
      "('I missed my flight QR404, what are my options?', {'entities': [(17, 22, 'FLIGHT_NUMBER')]})\n",
      "('Is there a meal on flight KE567?', {'entities': [(23, 28, 'FLIGHT_NUMBER')]})\n",
      "('How do I get to flight SQ738 now?', {'entities': [(20, 25, 'FLIGHT_NUMBER')]})\n",
      "('I want to cancel my booking on flight CX921.', {'entities': [(35, 40, 'FLIGHT_NUMBER')]})\n",
      "('What time does flight EK302 depart?', {'entities': [(19, 24, 'FLIGHT_NUMBER')]})\n",
      "('Flight TK403 has been rescheduled.', {'entities': [(7, 12, 'FLIGHT_NUMBER')]})\n",
      "('Please book me on the next flight, ideally AF1234.', {'entities': [(46, 52, 'FLIGHT_NUMBER')]})\n",
      "('Was flight NZ885 on time today?', {'entities': [(8, 13, 'FLIGHT_NUMBER')]})\n"
     ]
    }
   ],
   "source": [
    "with open('flight_number_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "trainset = [\n",
    "    (item['text'], {'entities': [(ent['start'], ent['end'], ent['label']) for ent in item['entities']]})\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "for entry in trainset:\n",
    "    print(entry)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that many pipes which are not required for the task are included into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at iteration 0: {'ner': 6.9482876346258164}\n",
      "Losses at iteration 1: {'ner': 4.702675337989926}\n",
      "Losses at iteration 2: {'ner': 5.156921973948253}\n",
      "Losses at iteration 3: {'ner': 5.153365394791363}\n",
      "Losses at iteration 4: {'ner': 5.00681877410164}\n",
      "Losses at iteration 5: {'ner': 6.293563479288324}\n",
      "Losses at iteration 6: {'ner': 4.653746864268369}\n",
      "Losses at iteration 7: {'ner': 3.537957224799879}\n",
      "Losses at iteration 8: {'ner': 5.216684199359153}\n",
      "Losses at iteration 9: {'ner': 3.3076572630249896}\n",
      "Losses at iteration 10: {'ner': 2.27047567395863}\n",
      "Losses at iteration 11: {'ner': 1.2480413240373949}\n",
      "Losses at iteration 12: {'ner': 0.12401556093125235}\n",
      "Losses at iteration 13: {'ner': 0.059436622092052034}\n",
      "Losses at iteration 14: {'ner': 0.0004466773342912489}\n",
      "Losses at iteration 15: {'ner': 0.00014524895916286283}\n",
      "Losses at iteration 16: {'ner': 2.248160550573137e-05}\n",
      "Losses at iteration 17: {'ner': 0.0016211101630137716}\n",
      "Losses at iteration 18: {'ner': 1.0550689085426457e-06}\n",
      "Losses at iteration 19: {'ner': 0.00022806830088065088}\n"
     ]
    }
   ],
   "source": [
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "epochs = 20\n",
    "\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.create_optimizer()\n",
    "    for i in range(epochs):\n",
    "        random.shuffle(trainset)\n",
    "        losses = {}\n",
    "        for text, annotations in trainset:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            nlp.update([example], drop=0.5, sgd=optimizer, losses=losses)\n",
    "        print(f\"Losses at iteration {i}: {losses}\")\n",
    "\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "ner.to_disk(r'C:\\Users\\vikto\\Workspace\\GitRepos\\KPI\\NLP\\lab7\\saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite fairly small size of training data the accuracy is satisfactory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in 'How many times do I have to tell you? PM572 is delayed, come home to Olive Street PT15'\n",
      "PM572 FLIGHT_NUMBER\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_text = \"How many times do I have to tell you? PM572 is delayed, come home to Olive Street PT15\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Entities in '%s'\" % test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.training import Example\n",
    "\n",
    "with open(\"flights.json\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I want to find a one way flight.', 'SearchOnewayFlight'),\n",
       " ('I am flying to Phoenix, AZ. I want 1 ticket from Seattle, WA.',\n",
       "  'SearchOnewayFlight'),\n",
       " (\"I want to depart 11th of March. I don't care which airline.\",\n",
       "  'SearchOnewayFlight'),\n",
       " ('A different airline please, in Premium Economy.', 'SearchOnewayFlight'),\n",
       " ('Find me something else. I want to fly with Southwest Airlines. Look for them from Atlanta.',\n",
       "  'SearchOnewayFlight'),\n",
       " ('Alright.', 'SearchOnewayFlight'),\n",
       " ('No thanks for your help.', 'NONE'),\n",
       " ('I would like to find a one way flight. I am interested in flights from Vegas.',\n",
       "  'SearchOnewayFlight'),\n",
       " ('I will be heading to Toronto, Canada. I would like to start my travel on the 13th of March.',\n",
       "  'SearchOnewayFlight'),\n",
       " ('What is the airport the flight will be landing at?', 'SearchOnewayFlight')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_data = []\n",
    "for dialogue in data:\n",
    "    for turn in dialogue[\"turns\"]:\n",
    "        if turn[\"speaker\"] == \"USER\":\n",
    "            utterance = turn[\"utterance\"]\n",
    "            intent = turn[\"frames\"][0][\"state\"][\"active_intent\"]\n",
    "            train_data.append((utterance, intent))\n",
    "\n",
    "train_data[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'NONE', 'ReserveOnewayFlight', 'SearchOnewayFlight'}, 744)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([element[1] for element in train_data]), len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "textcat = nlp.add_pipe(\"textcat\", last=True)\n",
    "textcat.add_label(\"SearchOnewayFlight\")\n",
    "textcat.add_label(\"NONE\")\n",
    "textcat.add_label(\"ReserveOnewayFlight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the data into the format spaCy accepts before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_annotation': {'cats': {'SearchOnewayFlight': True, 'NONE': False, 'ReserveOnewayFlight': False}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['I', 'want', 'to', 'find', 'a', 'one', 'way', 'flight', '.'], 'SPACY': [True, True, True, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8], 'DEP': ['', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0]}},\n",
       " {'doc_annotation': {'cats': {'SearchOnewayFlight': True, 'NONE': False, 'ReserveOnewayFlight': False}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['I', 'am', 'flying', 'to', 'Phoenix', ',', 'AZ', '.', 'I', 'want', '1', 'ticket', 'from', 'Seattle', ',', 'WA', '.'], 'SPACY': [True, True, True, True, False, True, False, True, True, True, True, True, True, False, True, False, False], 'TAG': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], 'DEP': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}},\n",
       " {'doc_annotation': {'cats': {'SearchOnewayFlight': True, 'NONE': False, 'ReserveOnewayFlight': False}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['I', 'want', 'to', 'depart', '11th', 'of', 'March', '.', 'I', 'do', \"n't\", 'care', 'which', 'airline', '.'], 'SPACY': [True, True, True, True, True, True, False, True, True, False, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'DEP': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}},\n",
       " {'doc_annotation': {'cats': {'SearchOnewayFlight': True, 'NONE': False, 'ReserveOnewayFlight': False}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['A', 'different', 'airline', 'please', ',', 'in', 'Premium', 'Economy', '.'], 'SPACY': [True, True, True, False, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8], 'DEP': ['', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0]}},\n",
       " {'doc_annotation': {'cats': {'SearchOnewayFlight': True, 'NONE': False, 'ReserveOnewayFlight': False}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['Find', 'me', 'something', 'else', '.', 'I', 'want', 'to', 'fly', 'with', 'Southwest', 'Airlines', '.', 'Look', 'for', 'them', 'from', 'Atlanta', '.'], 'SPACY': [True, True, True, False, True, True, True, True, True, True, True, False, True, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18], 'DEP': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}},\n",
       " {'doc_annotation': {'cats': {'SearchOnewayFlight': True, 'NONE': False, 'ReserveOnewayFlight': False}, 'entities': ['O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['Alright', '.'], 'SPACY': [False, False], 'TAG': ['', ''], 'LEMMA': ['', ''], 'POS': ['', ''], 'MORPH': ['', ''], 'HEAD': [0, 1], 'DEP': ['', ''], 'SENT_START': [1, 0]}},\n",
       " {'doc_annotation': {'cats': {'SearchOnewayFlight': False, 'NONE': True, 'ReserveOnewayFlight': False}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['No', 'thanks', 'for', 'your', 'help', '.'], 'SPACY': [True, True, True, True, False, False], 'TAG': ['', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', ''], 'POS': ['', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5], 'DEP': ['', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0]}},\n",
       " {'doc_annotation': {'cats': {'SearchOnewayFlight': True, 'NONE': False, 'ReserveOnewayFlight': False}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['I', 'would', 'like', 'to', 'find', 'a', 'one', 'way', 'flight', '.', 'I', 'am', 'interested', 'in', 'flights', 'from', 'Vegas', '.'], 'SPACY': [True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], 'DEP': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}},\n",
       " {'doc_annotation': {'cats': {'SearchOnewayFlight': True, 'NONE': False, 'ReserveOnewayFlight': False}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['I', 'will', 'be', 'heading', 'to', 'Toronto', ',', 'Canada', '.', 'I', 'would', 'like', 'to', 'start', 'my', 'travel', 'on', 'the', '13th', 'of', 'March', '.'], 'SPACY': [True, True, True, True, True, False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], 'DEP': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}},\n",
       " {'doc_annotation': {'cats': {'SearchOnewayFlight': True, 'NONE': False, 'ReserveOnewayFlight': False}, 'entities': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['What', 'is', 'the', 'airport', 'the', 'flight', 'will', 'be', 'landing', 'at', '?'], 'SPACY': [True, True, True, True, True, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'DEP': ['', '', '', '', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples = []\n",
    "for text, label in train_data:\n",
    "    cat = {'cats': {\n",
    "        \"SearchOnewayFlight\": label == \"SearchOnewayFlight\",\n",
    "        \"NONE\": label == \"NONE\",\n",
    "        \"ReserveOnewayFlight\": label == \"ReserveOnewayFlight\"\n",
    "    }}\n",
    "    train_examples.append(Example.from_dict(nlp.make_doc(text), cat))\n",
    "    \n",
    "train_examples[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses at iteration 0: {'textcat': 31.91867504242873}\n",
      "Losses at iteration 1: {'textcat': 34.66760695278206}\n",
      "Losses at iteration 2: {'textcat': 35.396802616697116}\n",
      "Losses at iteration 3: {'textcat': 34.29862304196533}\n",
      "Losses at iteration 4: {'textcat': 30.782300744441606}\n",
      "Losses at iteration 5: {'textcat': 29.546233219329565}\n",
      "Losses at iteration 6: {'textcat': 26.898122131081905}\n",
      "Losses at iteration 7: {'textcat': 26.116174917600972}\n",
      "Losses at iteration 8: {'textcat': 23.246139856456722}\n",
      "Losses at iteration 9: {'textcat': 19.77220083573154}\n",
      "Losses at iteration 10: {'textcat': 19.64046903320241}\n",
      "Losses at iteration 11: {'textcat': 18.357357815298116}\n",
      "Losses at iteration 12: {'textcat': 16.028874019542968}\n",
      "Losses at iteration 13: {'textcat': 14.493248600514107}\n",
      "Losses at iteration 14: {'textcat': 13.161071026858847}\n"
     ]
    }
   ],
   "source": [
    "optimizer = nlp.begin_training()\n",
    "for i in range(15):\n",
    "    losses = {}\n",
    "    batches = minibatch(train_examples, size=compounding(4., 32., 1.001))\n",
    "    for batch in batches:\n",
    "        nlp.update(batch, sgd=optimizer, drop=0.5, losses=losses)\n",
    "    print(f\"Losses at iteration {i}: {losses}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need a flight from New York to London next Monday. {'SearchOnewayFlight': 0.0009650694555602968, 'NONE': 2.225606112915557e-05, 'ReserveOnewayFlight': 0.9990127086639404}\n"
     ]
    }
   ],
   "source": [
    "test_text = \"I need a flight from New York to London next Monday.\"\n",
    "doc = nlp(test_text)\n",
    "print(test_text, doc.cats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Висновок:\n",
    "В ході виконання даної лабораторної роботи я ознайомився з основами роботи з бібліотекою spaCy для обробки та аналізу текстових даних. Зокрема, я навчився створювати власні набори даних у форматі JSON для розпізнавання нових типів сутностей і використовувати ці дані для навчання моделі NER (Named Entity Recognition). Це дозволило розширити функціональність стандартної моделі, додаючи здатність виявляти і класифікувати номери рейсів у текстах.\n",
    "\n",
    "Крім того, я застосував компонент TextCategorizer для класифікації текстів за намірами користувачів. Цей компонент допоміг мені структурувати діалоги користувачів зі штучним інтелектом для різних цілей, таких як пошук або бронювання рейсів. Я навчив модель розпізнавати різні типи запитів, базуючись на вмісті утерансів користувачів, що може бути корисним для подальшої розробки чат-ботів або систем автоматичної відповіді.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magicEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
