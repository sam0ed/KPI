{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лабораторна робота №6, Обробка та аналіз текстових даних на Python, Варіант 14\n",
    "**Виконав**: студент групи ІП-11, Лошак Віктор Іванович<br>\n",
    "**Перевірив**: Юлія Тимофєєва Сергіївна<br>\n",
    "\n",
    "**Тема роботи**: Аналіз настроїв<br>\n",
    "**Мета роботи**: Ознайомитись  з  вирішенням  задачі аналізу  настроїв та базовими можливостями бібліотеки spaCy.\n",
    "\n",
    "04.04.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Завдання**:<br>\n",
    "1. У файлі twitter2.csv містяться дані в форматі: clean_text,category, де можливими значеннями category є: <br>\n",
    "    -1 – негативний коментар,<br>\n",
    "    0 – нейтральний коментар,<br>\n",
    "    1 – позитивний коментар.<br>\n",
    "Використати наївний байєсів класифікатор для sentiment analysis.<br>\n",
    "2. У файлі lab6-1.txt.<br>\n",
    " a) Знайти та вивести всі слова з тексту, які не є стоп-словами.<br>\n",
    " b) Знайти та вивести всі прикметники, які присутні у тексті.<br>\n",
    " c) Знайти та вивести організації та дати, які присутні у тексті.<br>\n",
    "\n",
    "**Task**:<br>\n",
    "1. The file twitter2.csv contains data in the format: clean_text,category, where the possible values of category are: <br>\n",
    "     -1 - negative comment,<br>\n",
    "     0 - neutral comment,<br>\n",
    "     1 - a positive comment.<br>\n",
    "Use a naive Bayesian classifier for sentiment analysis.<br>\n",
    "2. In the file lab6-1.txt.<br>\n",
    "  a) Find and extract all words from the text that are not stop words.<br>\n",
    "  b) Find and display all the adjectives that are present in the text.<br>\n",
    "  c) Find and display organizations and dates that are present in the text.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162975</th>\n",
       "      <td>why these 456 crores paid neerav modi not reco...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162976</th>\n",
       "      <td>dear rss terrorist payal gawar what about modi...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162977</th>\n",
       "      <td>did you cover her interaction forum where she ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162978</th>\n",
       "      <td>there big project came into india modi dream p...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162979</th>\n",
       "      <td>have you ever listen about like gurukul where ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162980 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clean_text  category\n",
       "0       when modi promised “minimum government maximum...      -1.0\n",
       "1       talk all the nonsense and continue all the dra...       0.0\n",
       "2       what did just say vote for modi  welcome bjp t...       1.0\n",
       "3       asking his supporters prefix chowkidar their n...       1.0\n",
       "4       answer who among these the most powerful world...       1.0\n",
       "...                                                   ...       ...\n",
       "162975  why these 456 crores paid neerav modi not reco...      -1.0\n",
       "162976  dear rss terrorist payal gawar what about modi...      -1.0\n",
       "162977  did you cover her interaction forum where she ...       0.0\n",
       "162978  there big project came into india modi dream p...       0.0\n",
       "162979  have you ever listen about like gurukul where ...       1.0\n",
       "\n",
       "[162980 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('twitter2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_text    4\n",
       "category      7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162975</th>\n",
       "      <td>why these 456 crores paid neerav modi not reco...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162976</th>\n",
       "      <td>dear rss terrorist payal gawar what about modi...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162977</th>\n",
       "      <td>did you cover her interaction forum where she ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162978</th>\n",
       "      <td>there big project came into india modi dream p...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162979</th>\n",
       "      <td>have you ever listen about like gurukul where ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162969 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clean_text  category\n",
       "0       when modi promised “minimum government maximum...      -1.0\n",
       "1       talk all the nonsense and continue all the dra...       0.0\n",
       "2       what did just say vote for modi  welcome bjp t...       1.0\n",
       "3       asking his supporters prefix chowkidar their n...       1.0\n",
       "4       answer who among these the most powerful world...       1.0\n",
       "...                                                   ...       ...\n",
       "162975  why these 456 crores paid neerav modi not reco...      -1.0\n",
       "162976  dear rss terrorist payal gawar what about modi...      -1.0\n",
       "162977  did you cover her interaction forum where she ...       0.0\n",
       "162978  there big project came into india modi dream p...       0.0\n",
       "162979  have you ever listen about like gurukul where ...       1.0\n",
       "\n",
       "[162969 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df.dropna()\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.62      0.68      7152\n",
      "     Neutral       0.92      0.60      0.73     11067\n",
      "    Positive       0.68      0.92      0.78     14375\n",
      "\n",
      "    accuracy                           0.75     32594\n",
      "   macro avg       0.78      0.71      0.73     32594\n",
      "weighted avg       0.78      0.75      0.74     32594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_clean = df_clean['clean_text']\n",
    "y_clean = df_clean['category']\n",
    "\n",
    "X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)\n",
    "model_clean = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "model_clean.fit(X_train_clean, y_train_clean)\n",
    "y_pred_clean = model_clean.predict(X_test_clean)\n",
    "report_clean = classification_report(y_test_clean, y_pred_clean, target_names=['Negative', 'Neutral', 'Positive'])\n",
    "\n",
    "print(report_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 4409,   296,  2447],\n",
       "        [  652,  6672,  3743],\n",
       "        [  790,   320, 13265]], dtype=int64),\n",
       " 0.7469472909124378)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix_clean = confusion_matrix(y_test_clean, y_pred_clean)\n",
    "accuracy_clean = accuracy_score(y_test_clean, y_pred_clean)\n",
    "\n",
    "(conf_matrix_clean, accuracy_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to categorize sentiment based on TextBlob polarity score\n",
    "def categorize_sentiment(text):\n",
    "    sentiment = TextBlob(text).sentiment.polarity\n",
    "    if sentiment < 0:\n",
    "        return -1\n",
    "    elif sentiment == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[35509,     0,     0],\n",
       "        [    0, 55211,     0],\n",
       "        [    0,     2, 72247]], dtype=int64),\n",
       " 0.9999877277273592)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textblob_df = df_clean.copy()\n",
    "# Applying TextBlob sentiment analysis to the dataset\n",
    "textblob_df['textblob_category'] = textblob_df['clean_text'].apply(categorize_sentiment)\n",
    "\n",
    "# Calculating the confusion matrix and accuracy for TextBlob\n",
    "y_true_tb = textblob_df['category']\n",
    "y_pred_tb = textblob_df['textblob_category']\n",
    "\n",
    "conf_matrix_tb = confusion_matrix(y_true_tb, y_pred_tb)\n",
    "accuracy_tb = accuracy_score(y_true_tb, y_pred_tb)\n",
    "\n",
    "(conf_matrix_tb, accuracy_tb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "Printing all words that are not stop words. Printing all adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'US retail sales fell in January the biggest monthly decline since last August driven down by a heavy fall in car sales The fall in car sales had been expected coming after December 4 rise in car sales fuelled by generous special offers Excluding the car sector US retail sales were up in January twice what some analysts had been expecting US retail spending is expected to rise in 2005 but not as quickly as in 2004 Steve Gallagher US chief economist at SG Corporate Investment Banking said January figures were decent numbers We are not seeing the numbers that we saw in the second half of 2004 but they are still pretty healthy he added Sales at appliance and electronic stores were down in January while sales at hardware stores dropped by and furniture store sales dipped Sales at clothing and clothing accessory stores jumped while sales at general merchandise stores a category that includes department stores rose by These strong gains were in part put down to consumers spending gift vouchers they had been given for Christmas Sales at restaurants bars and coffee houses rose by while grocery store sales were up In December overall retail sales rose by Excluding the car sector sales rose by just Parul Jain deputy chief economist at Nomura Securities International said consumer spending would continue to rise in 2005 only at a slower rate of growth than in 2004 Consumers continue to retain their strength in the first quarter he said Van Rourke a bond strategist at Popular Securities agreed that the latest retail sales figures were slightly stronger than expected'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('lab6-1.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "    text = word_tokenize(text)\n",
    "    text = ' '.join([w for w in text if w.isalnum() ])\n",
    "    \n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non stop words:  ['US', 'retail', 'sales', 'fell', 'January', 'biggest', 'monthly', 'decline', 'August', 'driven', 'heavy', 'fall', 'car', 'sales', 'The', 'fall', 'car', 'sales', 'expected', 'coming', 'December', '4', 'rise', 'car', 'sales', 'fuelled', 'generous', 'special', 'offers', 'Excluding', 'car', 'sector', 'US', 'retail', 'sales', 'January', 'twice', 'analysts', 'expecting', 'US', 'retail', 'spending', 'expected', 'rise', '2005', 'quickly', '2004', 'Steve', 'Gallagher', 'US', 'chief', 'economist', 'SG', 'Corporate', 'Investment', 'Banking', 'said', 'January', 'figures', 'decent', 'numbers', 'We', 'seeing', 'numbers', 'saw', 'second', 'half', '2004', 'pretty', 'healthy', 'added', 'Sales', 'appliance', 'electronic', 'stores', 'January', 'sales', 'hardware', 'stores', 'dropped', 'furniture', 'store', 'sales', 'dipped', 'Sales', 'clothing', 'clothing', 'accessory', 'stores', 'jumped', 'sales', 'general', 'merchandise', 'stores', 'category', 'includes', 'department', 'stores', 'rose', 'These', 'strong', 'gains', 'consumers', 'spending', 'gift', 'vouchers', 'given', 'Christmas', 'Sales', 'restaurants', 'bars', 'coffee', 'houses', 'rose', 'grocery', 'store', 'sales', 'In', 'December', 'overall', 'retail', 'sales', 'rose', 'Excluding', 'car', 'sector', 'sales', 'rose', 'Parul', 'Jain', 'deputy', 'chief', 'economist', 'Nomura', 'Securities', 'International', 'said', 'consumer', 'spending', 'continue', 'rise', '2005', 'slower', 'rate', 'growth', '2004', 'Consumers', 'continue', 'retain', 'strength', 'quarter', 'said', 'Van', 'Rourke', 'bond', 'strategist', 'Popular', 'Securities', 'agreed', 'latest', 'retail', 'sales', 'figures', 'slightly', 'stronger', 'expected']\n",
      "Adjectives:  ['retail', 'biggest', 'monthly', 'last', 'heavy', 'generous', 'special', 'retail', 'retail', 'chief', 'decent', 'second', 'healthy', 'electronic', 'general', 'strong', 'overall', 'retail', 'deputy', 'chief', 'slower', 'first', 'latest', 'retail', 'stronger']\n",
      "Organisations:  ['SG Corporate Investment Banking', 'Nomura Securities International', 'Consumers', 'Popular Securities']\n",
      "Dates:  ['January', 'monthly', 'last August', 'December 4', 'January', '2005', '2004', 'January', 'the second half of 2004', 'January', 'Christmas', 'December', '2005', '2004', 'the first quarter']\n"
     ]
    }
   ],
   "source": [
    "# a) Extract words that are not stop words\n",
    "non_stop_words = [token.text for token in doc if token.text not in STOP_WORDS]\n",
    "\n",
    "# b) Find and display all adjectives\n",
    "adjectives = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "\n",
    "# c) Find and display organizations and dates present in the text\n",
    "organizations = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
    "dates = [ent.text for ent in doc.ents if ent.label_ == \"DATE\"]\n",
    "\n",
    "print(\"Non stop words: \", non_stop_words),\n",
    "print(\"Adjectives: \", adjectives),\n",
    "print(\"Organisations: \", organizations),\n",
    "print(\"Dates: \", dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Висновок:\n",
    "В ході виконання даної лабораторної роботи я ознайомився з основами аналізу настроїв у текстових даних за допомогою мови програмування Python та бібліотеки spaCy. Завдання полягало в аналізі настроїв в даних з Twitter та обробці тексту з файлу, що включало визначення настрою коментарів, виявлення слів, що не є стоп-словами, прикметників, організацій та дат. Використання наївного байєсового класифікатора дозволило провести класифікацію коментарів на негативні, нейтральні та позитивні з достатньою точністю, що демонструє ефективність цього методу для аналізу настроїв. Результати роботи показали, що методи обробки та аналізу текстових даних можуть бути ефективно застосовані для вирішення практичних завдань, таких як аналіз настроїв. Виконання цієї лабораторної роботи дало мені цінний досвід роботи з текстовими даними та їх аналізу, що буде корисним у моїй подальшій професійній діяльності."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pip_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
