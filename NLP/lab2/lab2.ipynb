{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лабораторна робота №2, Обробка та аналіз текстових даних на Python, Варіант 14\n",
    "**Виконав**: студент групи ІП-11, Лошак Віктор Іванович<br>\n",
    "**Перевірив**: Юлія Тимофєєва Сергіївна<br>\n",
    "\n",
    "**Тема роботи**: Попередня обробка тексту за допомогою NLTK<br>\n",
    "**Мета роботи**: Ознайомитись  з  основними  операціями  з  попередньої обробки тексту та їх реалізацією у бібліотеці NLTK.\n",
    "\n",
    "15.03.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Завдання**:\n",
    "1. Зчитати файл text4.<br>\n",
    "**а)** Порахувати кількість речень в тексті;<br>\n",
    "**б)** вивести 10 слів, які зустрічаються найчастіше;<br> \n",
    "**в)** провести лематизацію слів другого речення, попередньо визначивши частини мови.\n",
    "2. Використати корпус Brown, сьомий текст категорії adventure.<br>\n",
    "**а)** Видалити стоп-слова;<br> \n",
    "**б)** Вивести 8 іменників, що зустрічаються найчастіше.\n",
    "\n",
    "**Task**:\n",
    "1. Read the file text4.<br>\n",
    "**a)** Count the number of sentences in the text;<br>\n",
    "**b)** display 10 words that occur most often;<br>\n",
    "**c)** perform lemmatization of the words of the second sentence, having previously determined the parts of speech.\n",
    "2. Use the Brown corpus, the seventh text of the category adventure.<br>\n",
    "**a)** Remove stop words;<br>\n",
    "**b)** List 8 nouns that occur most often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“Then we had a talk as to what we should do, and Frank was all for\\nopenness, but I was so ashamed of it all that I felt as if I should\\nlike to vanish away and never see any of them again—just sending\\na line to pa, perhaps, to show him that I was alive. It was awful\\nto me to think of all those lords and ladies sitting round that\\nbreakfast-table and waiting for me to come back. So Frank took my\\nwedding-clothes and things and made a bundle of them, so that I should\\nnot be traced, and dropped them away somewhere where no one could find\\nthem. It is likely that we should have gone on to Paris to-morrow, only\\nthat this good gentleman, Mr. Holmes, came round to us this evening,\\nthough how he found us is more than I can think, and he showed us very\\nclearly and kindly that I was wrong and that Frank was right, and that\\nwe should be putting ourselves in the wrong if we were so secret. Then\\nhe offered to give us a chance of talking to Lord St. Simon alone, and\\nso we came right away round to his rooms at once. Now, Robert, you have\\nheard it all, and I am very sorry if I have given you pain, and I hope\\nthat you do not think very meanly of me.”\\n\\nLord St. Simon had by no means relaxed his rigid attitude, but had\\nlistened with a frowning brow and a compressed lip to this long\\nnarrative.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "with open('text4.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 6\n"
     ]
    }
   ],
   "source": [
    "# a) Count the number of sentences\n",
    "sentences = sent_tokenize(text)\n",
    "num_sentences = len(sentences)\n",
    "print(f\"Number of sentences: {num_sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most common words: [('and', 15), ('to', 13), ('i', 10), ('that', 10), ('we', 6), ('a', 6), ('was', 6), ('of', 6), ('should', 5), ('so', 5)]\n"
     ]
    }
   ],
   "source": [
    "# b) Display 10 words that occur most often\n",
    "words = word_tokenize(text.lower())\n",
    "filtered_words = [word for word in words if word.isalnum()]\n",
    "fdist = FreqDist(filtered_words)\n",
    "most_common_words = fdist.most_common(10)\n",
    "print(\"10 most common words:\", most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'), ('was', 'VBD'), ('awful', 'JJ'), ('to', 'TO'), ('me', 'PRP')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# c) Lemmatization of the words in the second sentence\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "second_sentence = sentences[1]\n",
    "second_sentence_tokens = word_tokenize(second_sentence)\n",
    "tagged_tokens = pos_tag(second_sentence_tokens)\n",
    "tagged_tokens[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На жаль pos_tag використовує систему тегування що не співпадає з тією яку використовує WordNetLemmatizer. Щоб виправити це використаємо функцію `get_wordnet_pos` для конвертації"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('It', 'PRP', 'n')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tokens[0][0], tagged_tokens[0][1], wn.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return wn.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second sentence: It was awful\n",
      "to me to think of all those lords and ladies sitting round that\n",
      "breakfast-table and waiting for me to come back.\n",
      "Lemmatized words of the second sentence: ['It', 'be', 'awful', 'to', 'me', 'to', 'think', 'of', 'all', 'those', 'lord', 'and', 'lady', 'sit', 'round', 'that', 'breakfast-table', 'and', 'wait', 'for', 'me', 'to', 'come', 'back', '.']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged_tokens]\n",
    "print(\"Second sentence:\", second_sentence)\n",
    "print(\"Lemmatized words of the second sentence:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The flat , hard cap was small , but he', 2403)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import pos_tag\n",
    "\n",
    "# a) Extract words from the seventh text of the 'adventure' category\n",
    "adventure_texts = brown.fileids(categories='adventure')\n",
    "seventh_text_id = adventure_texts[6]\n",
    "words = brown.words(fileids=seventh_text_id)\n",
    "' '.join(words[:10]), len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flat',\n",
       " 'hard',\n",
       " 'cap',\n",
       " 'small',\n",
       " 'thrust',\n",
       " 'back',\n",
       " 'head',\n",
       " 'tie',\n",
       " 'hell',\n",
       " 'could']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words = [word.lower() for word in words if word.lower() not in stopwords.words('english') and word.isalnum()]\n",
    "' '.join(filtered_words[:10])\n",
    "filtered_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 most common nouns: [('barton', 25), ('man', 11), ('dill', 11), ('hague', 9), ('rankin', 8), ('night', 7), ('valley', 7), ('kodyke', 7)]\n"
     ]
    }
   ],
   "source": [
    "# b) Count and list 8 most common nouns\n",
    "fdist = FreqDist(filtered_words)\n",
    "most_common_nouns = [(word, freq) for word, freq in fdist.most_common() if pos_tag([word])[0][1] == 'NN'][:8]\n",
    "print(\"8 most common nouns:\", most_common_nouns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Висновок:\n",
    "В ході виконання даної лабораторної роботи я ознайомився з основними методами попередньої обробки тексту та їх реалізацією за допомогою бібліотеки NLTK в Python. Лабораторна робота дала мені змогу практично застосувати процеси, такі як токенізація, видалення стоп-слів, лематизація та частотний аналіз слів, до реальних текстових даних.\n",
    "\n",
    "Я працював з корпусом Brown, конкретно з сьомим текстом категорії \"adventure\", що надало мені практичний досвід роботи з реальними текстовими корпусами та допомогло усвідомити важливість видалення стоп-слів для поліпшення якості подальшого аналізу.\n",
    "\n",
    "Лабораторна робота допомогла мені зрозуміти, як здійснюється підготовка тексту до аналізу та як можна ефективно використовувати бібліотеку NLTK для обробки та аналізу текстових даних в Python. Знання та навички, отримані в ході виконання цієї роботи, будуть корисні для майбутніх проектів, пов'язаних з обробкою та аналізом текстових даних."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
