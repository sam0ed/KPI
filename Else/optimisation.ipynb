{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code implements newtonian optimization on a surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, Eq, sin, solve, diff, lambdify\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_calls(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        wrapper.calls += 1\n",
    "        return func(*args, **kwargs)\n",
    "    wrapper.calls = 0\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the symbols\n",
    "x1, x2 = symbols('x1 x2')\n",
    "variables = [x1, x2]\n",
    "\n",
    "# Original equation\n",
    "eq1 = Eq(sin(2*x1 - x2) - 1.2*x1, 1.52)\n",
    "eq2 = Eq(0.8 * x1 + 1.5 * x2 , 1.83)\n",
    "\n",
    "\n",
    "eq1_transformed = Eq(eq1.lhs - eq1.rhs, 0)\n",
    "eq2_transformed = Eq(eq2.lhs - eq2.rhs, 0)\n",
    "\n",
    "cost_function = eq1_transformed.lhs**2 + eq2_transformed.lhs**2\n",
    "cost_function_lambda = count_calls(lambdify((x1, x2), cost_function, \"numpy\"))\n",
    "cost_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fig_config(fig):\n",
    "    fig.update_layout(title='Cost Function', autosize=False,\n",
    "                    width=500, height=500,\n",
    "                    margin=dict(l=65, r=50, b=65, t=90),\n",
    "                    scene=dict(\n",
    "                            xaxis_title='x1',\n",
    "                            yaxis_title='x2',\n",
    "                            zaxis_title='Cost'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x1 and x2 values\n",
    "x1_values = np.linspace(-15, 15, 600)\n",
    "x2_values = np.linspace(-15, 15, 600)\n",
    "\n",
    "# Generate a grid of points\n",
    "x1_grid, x2_grid = np.meshgrid(x1_values, x2_values)\n",
    "\n",
    "# Compute the cost function at each point on the grid\n",
    "Z = cost_function_lambda(x1_grid, x2_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Surface(z=Z, x=x1_grid, y=x2_grid)])\n",
    "fig_config(fig)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0 = [14, 12]\n",
    "a1 = [-10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Surface(z=Z, x=x1_grid, y=x2_grid)])\n",
    "fig.add_trace(go.Scatter3d(x=[a0[0]], y=[a0[1]], z=[cost_function_lambda(a0[0], a0[1])], mode='markers', name='a0'))\n",
    "fig.add_trace(go.Scatter3d(x=[a1[0]], y=[a1[1]], z=[cost_function_lambda(a1[0], a1[1])], mode='markers', name='a1'))\n",
    "fig_config(fig)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dichotomy_optimization(f, a, b, tolerance=1e-5, max_iterations=100):\n",
    "    if a >= b:\n",
    "        raise ValueError(\"Invalid interval: 'a' must be less than 'b'.\")\n",
    "    if tolerance <= 0:\n",
    "        raise ValueError(\"Tolerance must be positive.\")\n",
    "    \n",
    "    iteration = 0\n",
    "    while (b - a) / 2 > tolerance and iteration < max_iterations:\n",
    "        c = (a + b) / 2\n",
    "\n",
    "        epsilon = (b - a) / 4\n",
    "        left = c - epsilon\n",
    "        right = c + epsilon\n",
    "        \n",
    "        f_left = f(left)\n",
    "        f_right = f(right)\n",
    "\n",
    "        if f_left < f_right:\n",
    "            b = c  \n",
    "        else:\n",
    "            a = c \n",
    "        \n",
    "        iteration += 1\n",
    "    \n",
    "    return (a + b) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def golden_ratio_optimization(f, a, b, tolerance=1e-5, max_iterations=100):\n",
    "    if a >= b:\n",
    "        raise ValueError(\"Invalid interval: 'a' must be less than 'b'.\")\n",
    "    if tolerance <= 0:\n",
    "        raise ValueError(\"Tolerance must be positive.\")\n",
    "    \n",
    "    # The golden ratio\n",
    "    phi = (1 + 5 ** 0.5) / 2\n",
    "    \n",
    "    # Set the initial points\n",
    "    c = b - (b - a) / phi\n",
    "    d = a + (b - a) / phi\n",
    "    \n",
    "    iteration = 0\n",
    "    while (b - a) > tolerance and iteration < max_iterations:\n",
    "        # Evaluate the function at the points c and d\n",
    "        fc = f(c)\n",
    "        fd = f(d)\n",
    "        \n",
    "        # Narrow the interval\n",
    "        if fc < fd:\n",
    "            b = d\n",
    "            d = c\n",
    "            c = b - (b - a) / phi\n",
    "        else:\n",
    "            a = c\n",
    "            c = d\n",
    "            d = a + (b - a) / phi\n",
    "        \n",
    "        iteration += 1\n",
    "    \n",
    "    # The point where f(x) is minimized is in the middle of the final interval\n",
    "    return (b + a) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate_descent_step(current_point):\n",
    "    fixed_axis_ind = random.choice([0, 1])\n",
    "    working_axis_ind = (fixed_axis_ind + 1) % 2\n",
    "    print(f\"\\tWoking axis: {variables[working_axis_ind]}\")\n",
    "\n",
    "    # making our function one dimensional\n",
    "    cost_function_fixed = cost_function.subs(variables[fixed_axis_ind], current_point[fixed_axis_ind])\n",
    "    # evaluating the derivative at the current point\n",
    "    derivative_value = diff(cost_function_fixed, variables[working_axis_ind]).subs(\n",
    "        variables[working_axis_ind], current_point[working_axis_ind]).evalf()\n",
    "    \n",
    "    get_new_point = lambda step_size: [float(current_point[i] - step_size * derivative_value) if i == working_axis_ind else float(current_point[i]) for i in range(len(current_point))]\n",
    "\n",
    "    step_function_lambda = lambda l: cost_function_lambda(*get_new_point(l))\n",
    "    step_size = dichotomy_optimization(step_function_lambda, -10, 10, 0.01)\n",
    "\n",
    "    new_point = get_new_point(step_size)\n",
    "\n",
    "    return new_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step(initial_point):\n",
    "    var_value_dict = {var_name: initial_point[i] for i, var_name in enumerate(variables)}\n",
    "    gradient = [diff(cost_function, var).subs(var_value_dict).evalf() for var in variables]\n",
    "    # print('Gradient:', gradient)\n",
    "    \n",
    "    get_new_point = lambda step_size: [float(initial_point[i] - step_size * gradient[i]) for i in range(len(initial_point))]\n",
    "    step_function_lambda = lambda l: cost_function_lambda(*get_new_point(l))\n",
    "    \n",
    "    step_size = golden_ratio_optimization(step_function_lambda, -10, 10, 0.01)\n",
    "    new_point= get_new_point(step_size)\n",
    "    return new_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization(step_function, initial_point):\n",
    "    prev_point = initial_point.copy()\n",
    "    prev_point_cost = cost_function_lambda(*prev_point)\n",
    "    counter = 0\n",
    "    while True:\n",
    "        current_point = step_function(prev_point)\n",
    "        cost = cost_function_lambda(*current_point)\n",
    "        \n",
    "        if abs(prev_point_cost - cost) < 1e-9:\n",
    "            break\n",
    "        prev_point = current_point.copy()\n",
    "        prev_point_cost = cost     \n",
    "        fig.add_trace(go.Scatter3d(x=[current_point[0]], y=[current_point[1]], z=[cost], mode='markers', name=f'a0_{counter}'))\n",
    "        print(f'Iteration {counter}:')\n",
    "        print(f'\\tCurrent point: {current_point}')\n",
    "        print(f'\\tCost: {cost}')\n",
    "        counter+=1\n",
    "    \n",
    "    return current_point\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coordinate descent with random axis selection + Dichotomy method for selecting step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A0\n",
    "cost_function_lambda.calls = 0\n",
    "fig = go.Figure(data=[go.Surface(z=Z, x=x1_grid, y=x2_grid)])\n",
    "fig.add_trace(go.Scatter3d(x=[a0[0]], y=[a0[1]], z=[\n",
    "              cost_function_lambda(a0[0], a0[1])], mode='markers', name='a0'))\n",
    "optimization(coordinate_descent_step, a0)\n",
    "fig_config(fig)\n",
    "fig.show()\n",
    "print(cost_function_lambda.calls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function_lambda.calls = 0\n",
    "fig = go.Figure(data=[go.Surface(z=Z, x=x1_grid, y=x2_grid)])\n",
    "fig.add_trace(go.Scatter3d(x=[a1[0]], y=[a1[1]], z=[\n",
    "              cost_function_lambda(a1[0], a1[1])], mode='markers', name='a0'))\n",
    "optimization(coordinate_descent_step, a1)\n",
    "fig_config(fig)\n",
    "fig.show()\n",
    "print(cost_function_lambda.calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent + golden ratio method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function_lambda.calls = 0\n",
    "fig = go.Figure(data=[go.Surface(z=Z, x=x1_grid, y=x2_grid)])\n",
    "fig.add_trace(go.Scatter3d(x=[a0[0]], y=[a0[1]], z=[\n",
    "              cost_function_lambda(a0[0], a0[1])], mode='markers', name='a0'))\n",
    "optimization(gradient_descent_step, a0)\n",
    "fig_config(fig)\n",
    "fig.show()\n",
    "print(cost_function_lambda.calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function_lambda.calls = 0\n",
    "fig = go.Figure(data=[go.Surface(z=Z, x=x1_grid, y=x2_grid)])\n",
    "fig.add_trace(go.Scatter3d(x=[a1[0]], y=[a1[1]], z=[\n",
    "              cost_function_lambda(a1[0], a1[1])], mode='markers', name='a0'))\n",
    "optimization(gradient_descent_step, a1)\n",
    "fig_config(fig)\n",
    "fig.show()\n",
    "print(cost_function_lambda.calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the function\n",
    "def f(x):\n",
    "    return (np.sin(2*x[0]-x[1])-1.2*x[0]-1.52)**2 + (0.8*x[0]+1.5*x[1]-1.83)**2\n",
    "\n",
    "# Initial guess\n",
    "x0 = np.array([-1, 1])\n",
    "\n",
    "# Call the minimize function\n",
    "res = minimize(f, x0)\n",
    "\n",
    "print(f\"The minimum of the function is at x = {res.x}\")\n",
    "print(f\"The value of the function at this point is {res.fun}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
